{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd094af7",
   "metadata": {},
   "source": [
    "# <span style=\"font-width:bold; font-size: 3rem; color:#1EB182;\"><img src=\"../../images/icon102.png\" width=\"38px\"></img> **Hopsworks Feature Store** </span><span style=\"font-width:bold; font-size: 3rem; color:#333;\">- Part 02: Feature Pipeline</span>\n",
    "\n",
    "## üóíÔ∏è This notebook is divided into the following sections:\n",
    "1. Parse Data\n",
    "2. Feature Group Insertion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dcc328",
   "metadata": {},
   "source": [
    "### <span style='color:#ff5f27'> üìù Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364e961e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from features import air_quality\n",
    "from functions import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d04cc5-6788-4a4c-9f87-c2e00b5fce49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Opening the 'target_cities.json' file in read mode using the 'with' statement\n",
    "with open('target_cities.json') as json_file:\n",
    "    # Loading the JSON data from the file and storing it in the 'target_cities' variable\n",
    "    target_cities = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d2261f-8907-44f4-9f1a-bd9ec5e1556f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Getting the current date\n",
    "today = datetime.date.today()\n",
    "\n",
    "# Displaying the current date and its string representation\n",
    "today, str(today)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d406b01d",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\"> üîÆ Connecting to Hopsworks Feature Store </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba3cb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hopsworks\n",
    "\n",
    "project = hopsworks.login()\n",
    "fs = project.get_feature_store() \n",
    "\n",
    "# Retrieve feature groups\n",
    "air_quality_fg = fs.get_feature_group(\n",
    "    name='air_quality',\n",
    "    version=1,\n",
    ")\n",
    "weather_fg = fs.get_feature_group(\n",
    "    name='weather',\n",
    "    version=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f61053-a8c0-48a7-afa4-0e8733d2a54a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459ee37e-7e74-4051-97f6-2e03f9cac9d8",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> üå´ Filling gaps in Air Quality data (PM2.5)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ae9dd9-ab28-41d1-8478-5af27b7f767e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read data from feature groups\n",
    "df_air_quality = air_quality_fg.read()\n",
    "df_weather = weather_fg.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03063bc6-b58f-47f4-bfc6-8020ec196478",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extracting the \"date\" and \"city_name\" columns from the 'df_air_quality' DataFrame\n",
    "# Grouping the data by \"city_name\" and finding the maximum date for each city\n",
    "last_dates_aq = df_air_quality[[\"date\", \"city_name\"]].groupby(\"city_name\").max()\n",
    "\n",
    "# Converting the date values to string format for consistency\n",
    "last_dates_aq.date = last_dates_aq.date.astype(str)\n",
    "\n",
    "# Creating a dictionary with city names as keys and their corresponding last updated date as values\n",
    "last_dates_aq = last_dates_aq.to_dict()[\"date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e868bdf-e91a-410a-b654-a315c605f3dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Accessing the last updated date for the city of Paris\n",
    "paris_last_date = last_dates_aq.get(\"Paris\", \"Not available\")\n",
    "\n",
    "# Accessing the last updated date for the city of Columbus\n",
    "columbus_last_date = last_dates_aq.get(\"Columbus\", \"Not available\")\n",
    "\n",
    "# Printing the results\n",
    "print(\"‚õ≥Ô∏è Last update for Paris:\", paris_last_date)\n",
    "print(\"‚õ≥Ô∏è Last update for Columbus:\", columbus_last_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c4ee8d-7f7e-4bd0-a97b-c3ac0d7db50f",
   "metadata": {},
   "source": [
    "### <span style='color:#ff5f27'>  üßôüèº‚Äç‚ôÇÔ∏è Parsing PM2.5 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112a7974-37cb-4195-bc71-328af428c491",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Storing the current time as the start time of the cell execution\n",
    "start_of_cell = time.time()\n",
    "\n",
    "# Creating an empty DataFrame to store raw air quality data\n",
    "df_aq_raw = pd.DataFrame()\n",
    "\n",
    "# Iterating through continents and cities in the 'target_cities' dictionary\n",
    "for continent in target_cities:\n",
    "    for city_name, coords in target_cities[continent].items():\n",
    "        # Retrieving air quality data using the 'get_aqi_data_from_open_meteo' function\n",
    "        # with specified parameters such as city name, coordinates, start date, and end date\n",
    "        df_ = get_aqi_data_from_open_meteo(\n",
    "            city_name=city_name,\n",
    "            coordinates=coords,\n",
    "            start_date=last_dates_aq[city_name],\n",
    "            end_date=str(today)\n",
    "        )\n",
    "        \n",
    "        # Concatenating the retrieved data with the existing 'df_aq_raw' DataFrame\n",
    "        # and resetting the index to ensure proper alignment\n",
    "        df_aq_raw = pd.concat([df_aq_raw, df_]).reset_index(drop=True)\n",
    "\n",
    "# Storing the current time as the end time of the cell execution\n",
    "end_of_cell = time.time()\n",
    "\n",
    "# Printing information about the execution, including the time taken\n",
    "print(\"-\" * 64)\n",
    "print(f\"Parsed new PM2.5 data for ALL locations up to {str(today)}.\")\n",
    "print(f\"Took {round(end_of_cell - start_of_cell, 2)} sec.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afdc6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aq_raw.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250d9daf-83fa-49f1-bcd8-4efaeb90b99c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:#ff5f27;\">üõ† Feature Engineering PM2.5</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140b468a-e0c2-44a1-8e44-4cf393407eca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Converting the 'date' column in the 'df_aq_update' DataFrame to datetime format\n",
    "df_aq_raw['date'] = pd.to_datetime(df_aq_raw['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc181a9-6183-45ec-aed2-8ee684e13b39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Applying a feature engineering function 'feature_engineer_aq' to the 'df_aq_update' DataFrame\n",
    "df_aq_update = air_quality.feature_engineer_aq(df_aq_raw)\n",
    "\n",
    "# Dropping rows with missing values in the 'df_aq_update' DataFrame\n",
    "df_aq_update = df_aq_update.dropna()\n",
    "df_aq_update.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0364873c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the total number of missing values in the 'df_aq_update' DataFrame\n",
    "df_aq_update.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f67c89-6b39-4748-b4be-6ed3c9d57f96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieving the dimensions (number of rows and columns) of the 'df_aq_update' DataFrame\n",
    "df_aq_update.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74f5622-6f57-47b9-ac0b-dfb6617847b2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a34c64-5b94-4c4f-b03d-14e12a106f25",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> üå¶ Filling gaps in Weather data</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46009853-160c-467e-abb0-3145d27c57dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extracting the \"date\" and \"city_name\" columns from the 'df_weather' DataFrame\n",
    "# Grouping the data by \"city_name\" and finding the maximum date for each city\n",
    "last_dates_weather = df_weather[[\"date\", \"city_name\"]].groupby(\"city_name\").max()\n",
    "\n",
    "# Converting the date values to string format for consistency\n",
    "last_dates_weather.date = last_dates_weather.date.astype(str)\n",
    "\n",
    "# Creating a dictionary with city names as keys and their corresponding last updated date as values\n",
    "last_dates_weather = last_dates_weather.to_dict()[\"date\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd15812-a3a9-488c-879e-181c7b815357",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style='color:#ff5f27'>  üßôüèº‚Äç‚ôÇÔ∏è Parsing Weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef027d28-3443-4c7c-9e85-783625301a14",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Storing the current time as the start time of the cell execution\n",
    "start_of_cell = time.time()\n",
    "\n",
    "# Creating an empty DataFrame to store raw weather data\n",
    "df_weather_update = pd.DataFrame()\n",
    "\n",
    "# Iterating through continents and cities in the 'target_cities' dictionary\n",
    "for continent in target_cities:\n",
    "    for city_name, coords in target_cities[continent].items():\n",
    "        # Retrieving weather data using the 'get_weather_data_from_open_meteo' function\n",
    "        # with specified parameters such as city name, coordinates, start date, end date, and forecast flag\n",
    "        df_ = get_weather_data_from_open_meteo(\n",
    "            city_name=city_name,\n",
    "            coordinates=coords,\n",
    "            start_date=last_dates_weather[city_name],\n",
    "            end_date=str(today),\n",
    "            forecast=True,\n",
    "        )\n",
    "        \n",
    "        # Concatenating the retrieved data with the existing 'df_weather_update' DataFrame\n",
    "        # and resetting the index to ensure proper alignment\n",
    "        df_weather_update = pd.concat([df_weather_update, df_]).reset_index(drop=True)\n",
    "\n",
    "# Dropping rows with missing values in the 'df_weather_update' DataFrame\n",
    "df_weather_update.dropna(inplace=True)\n",
    "\n",
    "# Storing the current time as the end time of the cell execution\n",
    "end_of_cell = time.time()\n",
    "\n",
    "# Printing information about the execution, including the time taken\n",
    "print(\"-\" * 64)\n",
    "print(f\"Parsed new weather data for ALL cities up to {str(today)}.\")\n",
    "print(f\"Took {round(end_of_cell - start_of_cell, 2)} sec.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bff400-a2fb-48a3-a07b-5bd2a0469cd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Converting the 'date' column in the 'df_aq_update' DataFrame to datetime format\n",
    "df_aq_update.date = pd.to_datetime(df_aq_update.date)\n",
    "\n",
    "# Converting the 'date' column in the 'df_weather_update' DataFrame to datetime format\n",
    "df_weather_update.date = pd.to_datetime(df_weather_update.date)\n",
    "\n",
    "# Creating a new column 'unix_time' in 'df_aq_update' by applying the 'convert_date_to_unix' function\n",
    "df_aq_update[\"unix_time\"] = df_aq_update[\"date\"].apply(convert_date_to_unix)\n",
    "\n",
    "# Creating a new column 'unix_time' in 'df_weather_update' by applying the 'convert_date_to_unix' function\n",
    "df_weather_update[\"unix_time\"] = df_weather_update[\"date\"].apply(convert_date_to_unix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11752b30-2f40-4668-9813-2a90199c62b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Converting the 'date' column in the 'df_aq_update' DataFrame to string format\n",
    "df_aq_update.date = df_aq_update.date.astype(str)\n",
    "\n",
    "# Converting the 'wind_direction_dominant' column in the 'df_weather_update' DataFrame to integer format\n",
    "df_weather_update.wind_direction_dominant = df_weather_update.wind_direction_dominant.astype('int')\n",
    "\n",
    "# Converting the 'date' column in the 'df_weather_update' DataFrame to string format\n",
    "df_weather_update.date = df_weather_update.date.astype(str)\n",
    "df_weather_update.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792dd383",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aef353d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:#ff5f27;\">‚¨ÜÔ∏è Uploading new data to the Feature Store</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81bb922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert new data\n",
    "air_quality_fg.insert(df_aq_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0c498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert new data\n",
    "weather_fg.insert(df_weather_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50c64a1",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\">‚è≠Ô∏è **Next:** Part 03: Training Pipeline\n",
    " </span> \n",
    "\n",
    "In the following notebook you will read from a feature group and create training dataset within the feature store\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "190ea7959a836f4799545ea0f3718ade3abee093b15861ffdc25233d6ab7050e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
